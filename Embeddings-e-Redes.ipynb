{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMSP42fLZGHGJDAxshGQEgX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"UMZBEyiZp2fs"},"outputs":[],"source":["'''\n","🔎 O que são Word Embeddings?\n","Word Embeddings transformam palavras em vetores densos e contínuos, onde o significado semântico é preservado pela distância entre os vetores.\n","\n","✅ Exemplo:\n","king - man + woman ≈ queen\n","\n","✅ Técnicas famosas:\n","Word2Vec\n","GloVe\n","FastText\n","Embedding Layer (Keras / Hugging Face)\n","\n","📚 Como funciona o Word Embedding em uma Rede Neural\n","📌 Pipeline Geral:\n","Tokenização → números\n","Embedding Layer → vetor denso\n","Rede Neural (MLP, CNN, RNN, LSTM) → aprende padrões\n","Classificação / Regressão\n","'''"]},{"cell_type":"code","source":[],"metadata":{"id":"wzxJgH0DqbgK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install keras\n","!pip install tensorflow"],"metadata":{"id":"UakXHkevqa3S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.utils import pad_sequences\n","\n","# Dataset de exemplo\n","texts = [\n","    \"Eu amo esse filme\",\n","    \"Esse filme é péssimo\",\n","    \"Que filme maravilhoso\",\n","    \"Horrível, não gostei\",\n","    \"Gostei muito\",\n","    \"Péssimo, muito ruim\"\n","]\n","labels = [1, 0, 1, 0, 1, 0]  # 1 = Positivo, 0 = Negativo\n","\n","# 1. Tokenização\n","tokenizer = Tokenizer(num_words=1000)\n","tokenizer.fit_on_texts(texts)\n","X = tokenizer.texts_to_sequences(texts)\n","\n","# 2. Padding\n","X = pad_sequences(X, maxlen=10)\n","\n","# 3. Construindo a Rede com Embedding\n","model = Sequential()\n","model.add(Embedding(input_dim=1000, output_dim=64, input_length=10))  # Word Embedding\n","model.add(LSTM(64))                                                  # Captura dependências\n","model.add(Dense(1, activation='sigmoid'))                            # Saída binária\n","\n","# 4. Compilação\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# 5. Treinamento\n","model.fit(X, np.array(labels), epochs=10, verbose=1)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rogDabviqUJ8","executionInfo":{"status":"ok","timestamp":1742912267302,"user_tz":180,"elapsed":16255,"user":{"displayName":"Igu Balb","userId":"11670726490149624802"}},"outputId":"93aadb55-deba-4b09-e86c-bc78e67ee8bc"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step - accuracy: 0.8333 - loss: 0.6906\n","Epoch 2/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.8333 - loss: 0.6883\n","Epoch 3/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8333 - loss: 0.6860\n","Epoch 4/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 1.0000 - loss: 0.6837\n","Epoch 5/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 0.6812\n","Epoch 6/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 1.0000 - loss: 0.6785\n","Epoch 7/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 1.0000 - loss: 0.6757\n","Epoch 8/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 1.0000 - loss: 0.6726\n","Epoch 9/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 1.0000 - loss: 0.6692\n","Epoch 10/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 1.0000 - loss: 0.6656\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.history.History at 0x7a3a763a7910>"]},"metadata":{},"execution_count":1}]},{"cell_type":"code","source":["# Teste com frase nova\n","new_text = [\"O filme é maravilhoso\"]\n","new_seq = tokenizer.texts_to_sequences(new_text)\n","new_seq = pad_sequences(new_seq, maxlen=10)\n","\n","# Predição de sentimento\n","pred = model.predict(new_seq)\n","print(pred)  # Resultado: valor próximo de 1 (positivo)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vt3oyVogqgyy","executionInfo":{"status":"ok","timestamp":1742912285887,"user_tz":180,"elapsed":533,"user":{"displayName":"Igu Balb","userId":"11670726490149624802"}},"outputId":"097b9c9c-76a4-4603-bae9-431800c8008b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 312ms/step\n","[[0.5119744]]\n"]}]},{"cell_type":"code","source":["# Teste com frase nova\n","new_text = [\"O filme é ruim\"]\n","new_seq = tokenizer.texts_to_sequences(new_text)\n","new_seq = pad_sequences(new_seq, maxlen=10)\n","\n","# Predição de sentimento\n","pred = model.predict(new_seq)\n","print(pred)  # Resultado: valor próximo de 1 (positivo)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7EIHzX5tqw5Y","executionInfo":{"status":"ok","timestamp":1742912303490,"user_tz":180,"elapsed":211,"user":{"displayName":"Igu Balb","userId":"11670726490149624802"}},"outputId":"a660f08e-4de0-41a6-c822-8271bbaf746d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n","[[0.4881724]]\n"]}]},{"cell_type":"code","source":["'''\n","💾 Resumo das Camadas\n","\n","Camada - Função\n","Embedding - Transforma cada palavra num vetor de 64 dimensões (pré-treinado ou aprendido do zero)\n","LSTM - Captura o significado considerando a sequência (memória de longo prazo)\n","Dense - Classifica como positivo (1) ou negativo (0)\n","\n","\n","🎯 Vantagens de usar Embedding + Rede Neural\n","✅ O modelo aprende representações semânticas das palavras;\n","✅ Não é necessário manualmente calcular TF-IDF ou BoW;\n","✅ Captura contexto e ordem das palavras (coisa que BoW não faz).\n","'''"],"metadata":{"id":"lMaL7-aWq29D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"bMEsxKyVrRVK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install gensim"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wM7Z-mjFrXQI","executionInfo":{"status":"ok","timestamp":1742912479272,"user_tz":180,"elapsed":18634,"user":{"displayName":"Igu Balb","userId":"11670726490149624802"}},"outputId":"b60ce7ab-bf05-48b2-9226-7df7ecd190dd"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gensim\n","  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n","Collecting numpy<2.0,>=1.18.5 (from gensim)\n","  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n","  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n","Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.14.1\n","    Uninstalling scipy-1.14.1:\n","      Successfully uninstalled scipy-1.14.1\n","Successfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"]}]},{"cell_type":"code","source":["!pip install requests"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mq6jnYYJre9R","executionInfo":{"status":"ok","timestamp":1742912601949,"user_tz":180,"elapsed":3448,"user":{"displayName":"Igu Balb","userId":"11670726490149624802"}},"outputId":"02534bd3-2ea2-4b28-c921-476de6acea3e"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n"]}]},{"cell_type":"code","source":["# ✅ Como plugar FastText em uma rede neural (Keras)\n","\n","import requests\n","\n","x = requests.get('https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.pt.300.vec.gz')\n","open('cc.pt.300.vec.gz', 'wb').write(x.content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E10QcLncrRSg","executionInfo":{"status":"ok","timestamp":1742912680704,"user_tz":180,"elapsed":45818,"user":{"displayName":"Igu Balb","userId":"11670726490149624802"}},"outputId":"b8848b06-ded3-41d2-f411-3835d88597e1"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1271093660"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["!pip install --upgrade numpy\n","!pip install --upgrade gensim"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EMSeZpJUsZxZ","executionInfo":{"status":"ok","timestamp":1742912741277,"user_tz":180,"elapsed":13858,"user":{"displayName":"Igu Balb","userId":"11670726490149624802"}},"outputId":"01cd87e0-d2cf-460e-b1ab-54f0dc5ffd73"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n","Collecting numpy\n","  Downloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.26.4\n","    Uninstalling numpy-1.26.4:\n","      Successfully uninstalled numpy-1.26.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.4 which is incompatible.\n","numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\n","tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-2.2.4\n","Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n","Collecting numpy<2.0,>=1.18.5 (from gensim)\n","  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n","Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n","Installing collected packages: numpy\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.2.4\n","    Uninstalling numpy-2.2.4:\n","      Successfully uninstalled numpy-2.2.4\n","Successfully installed numpy-1.26.4\n"]}]},{"cell_type":"code","source":["!pip install --upgrade numpy gensim numba tensorflow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rr4Ta0IUs6Mm","executionInfo":{"status":"ok","timestamp":1742913105387,"user_tz":180,"elapsed":9471,"user":{"displayName":"Igu Balb","userId":"11670726490149624802"}},"outputId":"1e148bf2-1273-425e-94e4-ccc45867945c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n","Collecting numpy\n","  Using cached numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n","Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (0.61.0)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.19.0)\n","Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n","Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba) (0.44.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n","Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.19.0)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n","Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n","Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.7)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"]}]},{"cell_type":"code","source":["# Passo 3 - Carregar o FastText no Python\n","\n","from gensim.models import KeyedVectors\n","\n","# Carrega o modelo FastText pré-treinado (em .vec ou .bin)\n","fasttext_model = KeyedVectors.load_word2vec_format('cc.pt.300.vec.gz', binary=False)\n"],"metadata":{"id":"ej1x9LnCsLfR","executionInfo":{"status":"ok","timestamp":1742913602616,"user_tz":180,"elapsed":449040,"user":{"displayName":"Igu Balb","userId":"11670726490149624802"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Passo 4 - Criar a Embedding Matrix\n","\n","import numpy as np\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.utils import pad_sequences\n","\n","# Seu dataset\n","texts = [\"Eu amo esse filme\", \"Esse filme é péssimo\", \"Que filme maravilhoso\", \"Horrível, não gostei\"]\n","labels = [1, 0, 1, 0]\n","\n","# Tokeniza o texto\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(texts)\n","word_index = tokenizer.word_index\n","vocab_size = len(word_index) + 1\n","\n","# Cria a matriz de embeddings\n","embedding_dim = 300\n","embedding_matrix = np.zeros((vocab_size, embedding_dim))\n","\n","for word, i in word_index.items():\n","    if word in fasttext_model:\n","        embedding_matrix[i] = fasttext_model[word]\n","    else:\n","        embedding_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim,))  # Caso não encontre\n"],"metadata":{"id":"N8BLkcIRsMas","executionInfo":{"status":"ok","timestamp":1742913616322,"user_tz":180,"elapsed":5996,"user":{"displayName":"Igu Balb","userId":"11670726490149624802"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Passo 5 - Construir o modelo usando essa embedding\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","\n","# Padroniza o input\n","sequences = tokenizer.texts_to_sequences(texts)\n","X = pad_sequences(sequences, maxlen=10)\n","\n","# Monta a rede neural com a Embedding pré-treinada\n","model = Sequential()\n","model.add(Embedding(input_dim=vocab_size,\n","                    output_dim=embedding_dim,\n","                    weights=[embedding_matrix],  # Pluga o FastText aqui\n","                    input_length=10,\n","                    trainable=False))  # Pode deixar False para não \"estragar\" o FastText\n","model.add(LSTM(64))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Treina o modelo\n","model.fit(X, np.array(labels), epochs=10, verbose=1)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hOCmaw_et76n","executionInfo":{"status":"ok","timestamp":1742913630890,"user_tz":180,"elapsed":3465,"user":{"displayName":"Igu Balb","userId":"11670726490149624802"}},"outputId":"f55c5a95-49c3-41c8-fa0e-f3f2ff1fa9ad"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.5000 - loss: 0.7032\n","Epoch 2/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.5000 - loss: 0.6888\n","Epoch 3/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 1.0000 - loss: 0.6748\n","Epoch 4/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 0.6610\n","Epoch 5/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 0.6471\n","Epoch 6/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 0.6331\n","Epoch 7/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 0.6187\n","Epoch 8/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 0.6040\n","Epoch 9/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 0.5887\n","Epoch 10/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 1.0000 - loss: 0.5729\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.history.History at 0x7e92d6a0f2d0>"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# Teste com frase nova\n","new_text = [\"O filme é maravilhoso\"]\n","new_seq = tokenizer.texts_to_sequences(new_text)\n","new_seq = pad_sequences(new_seq, maxlen=10)\n","\n","# Predição de sentimento\n","pred = model.predict(new_seq)\n","print(pred)  # Resultado: valor próximo de 1 (positivo)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yBnIaNKMwB9v","executionInfo":{"status":"ok","timestamp":1742913685588,"user_tz":180,"elapsed":238,"user":{"displayName":"Igu Balb","userId":"11670726490149624802"}},"outputId":"835c4285-42eb-478a-cfdf-ece8ccbcae5a"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step\n","[[0.44475886]]\n"]}]},{"cell_type":"code","source":["# Teste com frase nova\n","new_text = [\"O filme é péssimo\"]\n","new_seq = tokenizer.texts_to_sequences(new_text)\n","new_seq = pad_sequences(new_seq, maxlen=10)\n","\n","# Predição de sentimento\n","pred = model.predict(new_seq)\n","print(pred)  # Resultado: valor próximo de 1 (positivo)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kfXp3SxOwFEY","executionInfo":{"status":"ok","timestamp":1742913694282,"user_tz":180,"elapsed":23,"user":{"displayName":"Igu Balb","userId":"11670726490149624802"}},"outputId":"6ec341ab-bb7e-4a26-ba4b-ee438587bd9f"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","[[0.4218196]]\n"]}]},{"cell_type":"code","source":["'''\n","📌 Resumo - O que você tem agora\n","\n","✅ FastText plugado\n","✅ Embedding com 300 dimensões\n","✅ Rede LSTM aprendendo sobre a sequência\n","✅ Output de classificação (positivo ou negativo)\n","'''"],"metadata":{"id":"FCVIQIAHt-o2"},"execution_count":null,"outputs":[]}]}