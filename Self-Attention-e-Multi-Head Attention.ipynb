{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNNpCo3mYBGeGcV6hD1IRKO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"imlxVF5BWiVL","executionInfo":{"status":"ok","timestamp":1743007707408,"user_tz":180,"elapsed":66,"user":{"displayName":"Igu Balb","userId":"11670726490149624802"}},"outputId":"be061dcb-9082-4af3-8cd3-2b86d069d22b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Pesos de Atenção (softmax):\n"," [[0.26820899 0.341926   0.38986501]\n"," [0.25100881 0.35895866 0.39003253]\n"," [0.25670316 0.31516267 0.42813417]]\n","\n","Resultado da Atenção:\n"," [[0.38799283 0.53502878 0.13623257 0.75889379]\n"," [0.39492337 0.53118004 0.1382761  0.75593604]\n"," [0.39012207 0.53832171 0.12822185 0.75093991]]\n"]}],"source":["import numpy as np  # Importa a biblioteca NumPy para manipulação de arrays\n","\n","# Definindo a função Scaled Dot-Product Attention\n","def scaled_dot_product_attention(Q, K, V):\n","    \"\"\"\n","    Calcula a atenção usando Q, K e V.\n","    \"\"\"\n","    d_k = Q.shape[-1]  # Obtém a dimensão dos vetores de chave (K)\n","\n","    # Calcula o produto escalar entre Q e K transposto\n","    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n","\n","    # Aplica softmax para obter os pesos de atenção\n","    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n","\n","    # Multiplica os pesos pela matriz V para obter o vetor de saída\n","    output = np.dot(attention_weights, V)\n","\n","    return output, attention_weights  # Retorna a saída e os pesos de atenção\n","\n","# Simulando vetores Q, K e V para um exemplo simples\n","np.random.seed(0)  # Define a seed para reprodutibilidade\n","Q = np.random.rand(3, 4)  # 3 tokens, dimensão 4\n","K = np.random.rand(3, 4)  # 3 tokens, dimensão 4\n","V = np.random.rand(3, 4)  # 3 tokens, dimensão 4\n","\n","# Calcula a atenção\n","attention_output, attention_weights = scaled_dot_product_attention(Q, K, V)\n","\n","print(\"Pesos de Atenção (softmax):\\n\", attention_weights)  # Exibe os pesos de atenção\n","print(\"\\nResultado da Atenção:\\n\", attention_output)  # Exibe a saída da atenção\n"]}]}