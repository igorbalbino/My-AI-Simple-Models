# Modelos de Estudo e testes - Machine Learning

## ğŸ“Œ VisÃ£o Geral
Este repositÃ³rio contÃ©m um projeto de Rede Neural Recorrente (RNN) com unidades GRU (Gated Recurrent Unit) implementado em Python usando Jupyter Notebook. O modelo foi treinado com uma quantidade limitada de dados devido a restriÃ§Ãµes computacionais, e incorpora a mÃ©trica de Masked Loss para lidar com sequÃªncias de comprimento variÃ¡vel.

## ğŸ§  Tecnologias Utilizadas
Python 3.x
Jupyter Notebook
TensorFlow/Keras
Bibliotecas padrÃ£o para ML (NumPy, Pandas)

## ğŸš€ Como Executar o Modelo
PrÃ©-requisitos
Python 3.7 ou superior
TensorFlow 2.x instalado

## OpÃ§Ã£o 1: Carregar o modelo para inferÃªncia
```from tensorflow.keras.models import load_model

### Carregar o modelo prÃ©-treinado
model = load_model('caminho/para/seu/modelo.h5', compile=False)

### Configurar o modelo para inferÃªncia (ajuste conforme sua arquitetura)
encoder_model = Model(inputs=model.input[0], outputs=model.get_layer('encoder_outputs').output)
decoder_model = ... # Configurar o decoder para inferÃªncia```



## OpÃ§Ã£o 2: Continuar o treinamento
```from tensorflow.keras.models import load_model

### Carregar o modelo com opÃ§Ã£o para continuar treinamento
model = load_model('caminho/para/seu/modelo.h5')

### Compilar o modelo (especifique seu optimizer e loss)
model.compile(optimizer='adam', 
              loss='categorical_crossentropy',
              metrics=['accuracy'])

### Continuar treinamento
model.fit(novos_dados, novos_rotulos, epochs=10, batch_size=32)```

## ğŸ“ Notas Importantes
O arquivo .h5 contÃ©m:
Arquitetura do modelo Seq2Seq com atenÃ§Ã£o
Pesos treinados
ConfiguraÃ§Ãµes do otimizador (se salvou com compile=True)

Para usar corretamente:
Certifique-se de prÃ©-processar seus dados da mesma forma que durante o treinamento
O mecanismo de atenÃ§Ã£o requer configuraÃ§Ã£o especÃ­fica para inferÃªncia

Se encontrar erros ao carregar:

```# Pode ser necessÃ¡rio custom_objects se usou layers customizados
model = load_model('modelo.h5', custom_objects={'AttentionLayer': AttentionLayer})```