{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPtPvzE0r1JtsbP3H7TmT8I"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"NFQ8Ez0s_jDP"},"outputs":[],"source":["'''\n","‚úÖ Ordem ideal de uma pipeline de NLP para o seu caso:\n","\n","Pr√©-processamento b√°sico:\n","Normaliza√ß√£o (lowercase, remo√ß√£o de s√≠mbolos, etc.)\n","Tokeniza√ß√£o\n","Part-of-Speech (POS) Tagging\n","Lemmatiza√ß√£o (usando os POS tags para ser mais preciso)\n","Remo√ß√£o de Stopwords\n","Dependency Parsing\n","Word Embeddings ou vetoriza√ß√£o final (TF-IDF, BoW ou Embeddings)\n","'''\n","\n","#\n","\n","'''\n","üß† Por qu√™ essa ordem?\n","\n","Etapa\tPor qu√™ aqui?\n","\n","Tokeniza√ß√£o - Sem tokens, n√£o d√° pra fazer nada depois.\n","POS Tagging - Ajuda a melhorar a lematiza√ß√£o ‚Äî \"amando\" pode ser verbo ou substantivo.\n","Lemmatiza√ß√£o - Com POS, a lematiza√ß√£o fica mais precisa. Ex: \"melhores\" ‚Üí \"bom\" se for adjetivo.\n","Stopwords - Ap√≥s a lematiza√ß√£o, voc√™ remove palavras j√° na forma correta. Ex: \"fui\" ‚Üí \"ir\".\n","Dependency Parsing - Faz mais sentido com o texto limpo, sem stopwords, e tokens lematizados.\n","Word Embeddings - Sempre no final, para trabalhar com o texto vetorizado, pronto para machine learning ou deep learning.\n","'''"]},{"cell_type":"code","source":["!python -m spacy download pt_core_news_md"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EwgQJ0eZBU9g","executionInfo":{"status":"ok","timestamp":1742851146408,"user_tz":180,"elapsed":25158,"user":{"displayName":"Igu Balb","userId":"11670726490149624802"}},"outputId":"5ce04a8d-3c7e-4418-af73-666d54e8ca96"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pt-core-news-md==3.8.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_md-3.8.0/pt_core_news_md-3.8.0-py3-none-any.whl (42.4 MB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pt-core-news-md\n","Successfully installed pt-core-news-md-3.8.0\n","\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('pt_core_news_md')\n","\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n"]}]},{"cell_type":"code","source":["# Exemplo de Pipeline Completa (usando SpaCy - portugu√™s)\n","\n","import spacy\n","\n","# Carrega o modelo do spaCy para portugu√™s\n","nlp = spacy.load(\"pt_core_news_md\")\n","\n","# Exemplo de texto\n","texto = \"Os meninos estavam jogando futebol na pra√ßa ontem √† tarde.\"\n","\n","# Passa o texto pelo pipeline do spaCy (Tokeniza√ß√£o, POS, Parsing autom√°ticos)\n","doc = nlp(texto)\n","\n","# Lista para armazenar tokens finais ap√≥s lematiza√ß√£o e stopword removal\n","tokens_processados = []\n","\n","for token in doc:\n","    # Exibe informa√ß√µes b√°sicas de cada token\n","    print(f\"TOKEN: {token.text} | LEMMA: {token.lemma_} | POS: {token.pos_} | DEP: {token.dep_}\")\n","\n","    # Remove stopwords e pontua√ß√µes\n","    if not token.is_stop and not token.is_punct:\n","        # Lematiza o token\n","        tokens_processados.append(token.lemma_)\n","\n","print(\"\\nTokens finais ap√≥s lematiza√ß√£o e stopwords:\")\n","print(tokens_processados)\n","\n","# Dependency Parsing - visualiza a rela√ß√£o de depend√™ncia de cada token\n","for token in doc:\n","    print(f\"{token.text} <--{token.dep_}-- {token.head.text}\")\n","\n","# Embedding de cada token (vetor denso)\n","print(\"\\nExemplo de vetor do primeiro token √∫til:\")\n","print(tokens_processados[0], \":\", doc[0].vector)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X0Wpl1JgBHY5","executionInfo":{"status":"ok","timestamp":1742851251707,"user_tz":180,"elapsed":5689,"user":{"displayName":"Igu Balb","userId":"11670726490149624802"}},"outputId":"6ccc417b-d639-4deb-dfcb-a187c37d7171"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["TOKEN: Os | LEMMA: o | POS: DET | DEP: det\n","TOKEN: meninos | LEMMA: menino | POS: NOUN | DEP: nsubj\n","TOKEN: estavam | LEMMA: estar | POS: AUX | DEP: aux\n","TOKEN: jogando | LEMMA: jogar | POS: VERB | DEP: ROOT\n","TOKEN: futebol | LEMMA: futebol | POS: NOUN | DEP: obj\n","TOKEN: na | LEMMA: em o | POS: ADP | DEP: case\n","TOKEN: pra√ßa | LEMMA: pra√ßa | POS: NOUN | DEP: obl\n","TOKEN: ontem | LEMMA: ontem | POS: ADV | DEP: advmod\n","TOKEN: √† | LEMMA: a o | POS: ADP | DEP: case\n","TOKEN: tarde | LEMMA: tarde | POS: NOUN | DEP: obl\n","TOKEN: . | LEMMA: . | POS: PUNCT | DEP: punct\n","\n","Tokens finais ap√≥s lematiza√ß√£o e stopwords:\n","['menino', 'estar', 'jogar', 'futebol', 'pra√ßa']\n","Os <--det-- meninos\n","meninos <--nsubj-- jogando\n","estavam <--aux-- jogando\n","jogando <--ROOT-- jogando\n","futebol <--obj-- jogando\n","na <--case-- pra√ßa\n","pra√ßa <--obl-- jogando\n","ontem <--advmod-- jogando\n","√† <--case-- tarde\n","tarde <--obl-- ontem\n",". <--punct-- jogando\n","\n","Exemplo de vetor do primeiro token √∫til:\n","menino : [ -5.3293    10.735     15.596    -13.801     -8.5943     5.7797\n","   5.0652   -12.663    -10.053     -1.9764    -3.8597    -0.6195\n","  -1.294      5.7384    10.266     -2.47      32.385     -5.1625\n","  12.799      8.4848     5.0475     9.1212   -19.978     -1.9721\n"," -10.909     -9.4839     0.92546   -5.5222    -6.6009   -15.484\n","  -4.5628     8.9471     1.9523    -4.7423    -5.9634     1.8066\n","   5.5961    -7.6064    17.585      3.5194    10.607      3.2886\n"," -11.707     -3.3579     3.2146     4.4919    11.168      8.9138\n","   3.9884    -3.3426    -5.7893     3.9091   -11.206     -4.6497\n","  -8.3294     7.2746     4.6976    -8.9812    11.788     -7.6789\n","   5.6849    -8.159      0.8127     5.5593    13.727     -6.4899\n","   8.3682    -1.8921   -10.938      7.3867     4.4542    -5.9318\n","  -0.65349    4.1518   -10.111      4.0277    -0.52748   -5.9766\n","   3.6167    -7.3565   -10.891     10.112      1.0592    -0.63465\n","   2.1907   -12.351     -3.5995    -7.2003    -8.1459     6.4436\n"," -14.554     -9.6043    -5.9576    -7.0373     2.4695     7.408\n","  -2.2913     8.1851    16.984    -10.806     -5.7996    -5.5318\n","  -8.739     -7.7679     8.2805     1.6497     5.9443   -15.76\n","  -5.0274    11.089     -1.5608    -0.64239  -11.534     -0.998\n","   0.38343    1.8057     0.39256    3.7962    -2.7415    -6.0301\n","   0.66013    3.3849    -7.0399     1.5171     8.6415    -1.1231\n","   5.0489   -10.219     -2.4856     6.5035     2.0929     2.171\n","  -1.8657    -3.971     -6.1356    -6.5281   -13.571      8.065\n","  -2.8447    -5.7711     7.7675    -9.2566     8.3787   -10.842\n","   1.9788     0.85396  -11.508     -1.8994    14.661      1.4028\n","  -5.8076    -4.5444     3.849     17.219     11.096      0.54775\n","  -2.1879    21.524     -1.7421     2.6483    -2.4797     2.698\n","  -9.629     -2.4596     8.116      2.2314    -1.0019     2.1677\n","   0.088457 -11.129     -4.1894     0.5117    -1.7319     8.1133\n","  -6.009      2.0029     2.2388    10.291      5.8574     8.9127\n","   5.8543     5.9471     8.5652    -9.6047   -17.429      1.5278\n","   1.4512     8.937     -2.9823    -7.2264    -9.8626   -13.228\n","  -3.2176     6.5867     9.1171    -5.9545    -4.7731    -2.3195\n","   4.5401     4.7888     7.0545    -1.8568     6.7714     4.8989\n","  -3.5644     8.9673    -4.2576    -7.6053     1.0557     3.565\n","  -1.3875    -1.8814    -4.2066    -0.12017   11.922      1.1267\n","  -8.3074    10.024      5.1606     6.4999     2.0257    -4.799\n","   2.806      8.0768    -7.3777     6.9292     2.0996    -6.2524\n","  -7.5907    -2.4465     0.98656  -10.519     13.115      4.6218\n","  -3.0396    -3.4335     0.71105  -10.17       0.93342   13.862\n","  10.114    -17.985    -18.062      3.9462   -20.976      1.4343\n","  11.136    -16.777      0.51883   -7.1087    11.002     -4.3999\n","  -0.34174   -1.0791     4.9108     6.5083     9.7889     2.8073\n","   3.3984    -7.7238    -3.4626    -2.7186    -1.7094     2.0241\n","  19.697     -2.8515   -10.863     -2.6839     3.814      2.5043\n","   1.4642     4.4556     2.4212     6.6767     3.6385    -8.3212\n","  -2.7722    -2.8037    13.096     -5.9267    -0.58572    9.9115\n","   5.5574    -7.4085    -5.3371     0.053114   9.8543     4.3087\n","  -3.0675     4.543      0.27363    3.5598    -8.0474    -3.3163\n","  -0.9647    11.754     -6.3805    11.547     -5.6072    24.115   ]\n"]}]},{"cell_type":"code","source":["'''\n","üîé O que est√° acontecendo (linha por linha)\n","¬¥nlp = spacy.load(\"pt_core_news_md\")¬¥: Carrega o modelo pr√©-treinado de portugu√™s com embeddings.\n","¬¥doc = nlp(texto)¬¥: O texto passa pela pipeline padr√£o do SpaCy: tokeniza√ß√£o, POS tagging e dependency parsing.\n","\n","Loop em doc:\n","  Mostra o token original, sua lematiza√ß√£o, classe gramatical (POS) e depend√™ncia sint√°tica.\n","¬¥if not token.is_stop and not token.is_punct¬¥: Remove palavras irrelevantes e pontua√ß√£o.\n","¬¥.lemma_¬¥: Lematiza cada token para obter sua forma base.\n","¬¥token.dep_¬¥: Mostra a depend√™ncia sint√°tica, ou seja, quem depende de quem na frase.\n","¬¥token.vector¬¥: Retorna o vetor embedding denso para o token.\n","\n","---------- ---------- ----------\n","\n","‚úÖ Resultado final esperado\n","Lista limpa de tokens lematizados\n","Rela√ß√£o sint√°tica entre palavras (Dependency Parsing)\n","Vetores prontos para serem input em modelos de ML/DL\n","'''"],"metadata":{"id":"Hbu48WR1CA2p"},"execution_count":null,"outputs":[]}]}